"""BitFlowåŸºæœ¬ä½¿ç”¨ç¤ºä¾‹"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from bitflow.ops import conv2d


def example_basic_conv():
    """åŸºæœ¬å·ç§¯ç¤ºä¾‹"""
    print("=== åŸºæœ¬å·ç§¯ç¤ºä¾‹ ===")

    # åˆ›å»ºè¾“å…¥æ•°æ®
    batch_size = 1
    in_channels = 3
    height, width = 32, 32

    input_tensor = torch.randn(batch_size, in_channels, height, width)

    # åˆ›å»ºå·ç§¯å‚æ•°
    out_channels = 16
    kernel_size = 3
    weight = torch.randn(out_channels, in_channels, kernel_size, kernel_size)
    bias = torch.randn(out_channels)

    # ä½¿ç”¨BitFlowå·ç§¯
    output = conv2d(input_tensor, weight, bias, stride=1, padding=1)

    print(f"Input shape: {input_tensor.shape}")
    print(f"Weight shape: {weight.shape}")
    print(f"Output shape: {output.shape}")

    # éªŒè¯ä¸PyTorchçš„ä¸€è‡´æ€§
    pytorch_output = F.conv2d(input_tensor, weight, bias, stride=1, padding=1)
    max_diff = torch.max(torch.abs(output - pytorch_output))

    print(f"Max difference with PyTorch: {max_diff.item():.2e}")
    assert max_diff < 1e-4, "Results differ too much from PyTorch"
    print("âœ“ Results match PyTorch implementation")


def example_autograd():
    """è‡ªåŠ¨æ±‚å¯¼ç¤ºä¾‹"""
    print("\n=== è‡ªåŠ¨æ±‚å¯¼ç¤ºä¾‹ ===")

    # åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡
    input_tensor = torch.randn(1, 3, 32, 32, requires_grad=True)
    weight = torch.randn(16, 3, 3, 3, requires_grad=True)
    bias = torch.randn(16, requires_grad=True)

    # å‰å‘ä¼ æ’­
    output = conv2d(input_tensor, weight, bias, stride=1, padding=1)
    loss = output.sum()

    # åå‘ä¼ æ’­
    loss.backward()

    print(f"Input gradient shape: {input_tensor.grad.shape}")
    print(f"Weight gradient shape: {weight.grad.shape}")
    print(f"Bias gradient shape: {bias.grad.shape}")
    print("âœ“ Autograd working correctly")


def example_cuda():
    """CUDAç¤ºä¾‹"""
    if not torch.cuda.is_available():
        print("\n=== CUDA not available, skipping ===")
        return

    print("\n=== CUDAç¤ºä¾‹ ===")

    device = torch.device('cuda')

    # åˆ›å»ºCUDAå¼ é‡
    input_tensor = torch.randn(2, 64, 64, 64, device=device)
    weight = torch.randn(128, 64, 3, 3, device=device)
    bias = torch.randn(128, device=device)

    # ä½¿ç”¨BitFlow CUDAå·ç§¯
    output = conv2d(input_tensor, weight, bias)

    print(f"Input device: {input_tensor.device}")
    print(f"Output device: {output.device}")
    print(f"Output shape: {output.shape}")
    print("âœ“ CUDA computation successful")


def example_performance():
    """æ€§èƒ½æµ‹è¯•ç¤ºä¾‹"""
    print("\n=== æ€§èƒ½æµ‹è¯•ç¤ºä¾‹ ===")

    from bitflow.ops.conv import benchmark_conv2d
    benchmark_conv2d()


class BitFlowConvNet(nn.Module):
    """ä½¿ç”¨BitFlowå·ç§¯çš„ç®€å•ç½‘ç»œ"""

    def __init__(self, in_channels=3, num_classes=10):
        super().__init__()

        # å®šä¹‰å·ç§¯å±‚å‚æ•°
        self.conv1_weight = nn.Parameter(torch.randn(32, in_channels, 3, 3))
        self.conv1_bias = nn.Parameter(torch.randn(32))

        self.conv2_weight = nn.Parameter(torch.randn(64, 32, 3, 3))
        self.conv2_bias = nn.Parameter(torch.randn(64))

        # åˆ†ç±»å±‚
        self.classifier = nn.Linear(64 * 6 * 6, num_classes)

    def forward(self, x):
        # ç¬¬ä¸€ä¸ªå·ç§¯å±‚ + ReLU + MaxPool
        x = conv2d(x, self.conv1_weight, self.conv1_bias, padding=1)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)

        # ç¬¬äºŒä¸ªå·ç§¯å±‚ + ReLU + MaxPool
        x = conv2d(x, self.conv2_weight, self.conv2_bias, padding=1)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)

        # å±•å¹³å¹¶åˆ†ç±»
        x = x.view(x.size(0), -1)
        x = self.classifier(x)

        return x


def example_training():
    """è®­ç»ƒç¤ºä¾‹"""
    print("\n=== è®­ç»ƒç¤ºä¾‹ ===")

    # åˆ›å»ºç½‘ç»œå’Œä¼˜åŒ–å™¨
    model = BitFlowConvNet()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    # åˆ›å»ºè™šæ‹Ÿæ•°æ®
    batch_size = 4
    input_data = torch.randn(batch_size, 3, 28, 28)
    target = torch.randint(0, 10, (batch_size,))

    # è®­ç»ƒä¸€ä¸ªæ­¥éª¤
    optimizer.zero_grad()

    output = model(input_data)
    loss = criterion(output, target)

    print(f"Output shape: {output.shape}")
    print(f"Loss: {loss.item():.4f}")

    loss.backward()
    optimizer.step()

    print("âœ“ Training step completed successfully")


if __name__ == "__main__":
    print("BitFlow ä½¿ç”¨ç¤ºä¾‹\n")

    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    example_basic_conv()
    example_autograd()
    example_cuda()
    example_performance()
    example_training()

    print("\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡ŒæˆåŠŸï¼")
